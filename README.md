# ğŸµ Emotion-Based Music Player

Emotion based music recommendation system which detects your emotions in real time using facial expressions and hand gestures, then suggests music that matches your mood. Built with Python, OpenCV, MediaPipe, Keras, and Streamlit, this project bridges computer vision and deep learning to deliver a personalized musical experience.

---

## ğŸš€ **Features**

- ğŸ” **Real-time emotion detection** using webcam input
- ğŸ­ **Facial and hand landmark tracking** with MediaPipe
- ğŸ§  **Custom deep learning model** trained on your own emotion data
- ğŸµ **Music recommendations** based on emotion + language + singer
- ğŸŒ **Streamlit web app** interface with YouTube integration
- ğŸ“€ **Complete data collection and training pipeline included**

---

## ğŸ«  **Technologies Used**

- **Python 3** â€“ Core programming language
- **OpenCV** â€“ Webcam input and image processing
- **MediaPipe** â€“ Real-time face and hand landmark detection
- **NumPy** â€“ Data processing and storage
- **Keras (TensorFlow backend)** â€“ Deep learning model
- **Streamlit** â€“ Web app interface
- **streamlit-webrtc** â€“ Live webcam streaming in Streamlit

---

## ğŸ”§ **Modules Overview**

### ğŸ“… `data_collection.py` â€“ *Data Collection Module*

This script captures real-time landmark data from webcam for training:

- Captures **face and hand landmarks** using MediaPipe Holistic
- Normalizes coordinates relative to key points
- Records 100 frames of expression data per session
- Saves as `.npy` file named after the emotion (e.g., `happy.npy`)

### ğŸ§  `data_training.py` â€“ *Model Training Module*

Trains a neural network model using the collected landmark data:

- Loads all `.npy` files in the directory
- Labels and one-hot encodes the data
- Shuffles and trains a deep neural network
- Saves the trained model as `model1.h5` and emotion labels as `labels.npy`

### ğŸ” `inference.py` â€“ *Real-Time Emotion Detection*

Detects emotion live using the webcam and trained model:

- Loads `model1.h5` and `labels.npy`
- Extracts and preprocesses facial and hand landmarks in real time
- Displays predicted emotion on video feed

### ğŸ¶ `music.py` â€“ *Streamlit Web App for Music Recommendation*

Interactive app that detects your emotion and recommends music:

- Built with **Streamlit** and **streamlit-webrtc** for live video
- Detects emotion and saves it in `emotion.npy`
- Accepts user inputs for **language** and **singer**
- Opens **YouTube** with relevant search like: `happy English song Arijit Singh`

---

## ğŸš§ **Setup Instructions**

### 1. **Install Dependencies**
```bash
pip install opencv-python mediapipe numpy keras streamlit streamlit-webrtc
```

### 2. **Run Data Collection**
```bash
python data_collection.py
```

### 3. **Train the Model**
```bash
python data_training.py
```

### 4. **Run Inference Script (Optional)**
```bash
python inference.py
```

### 5. **Launch Streamlit App**
```bash
streamlit run music.py
```

---

## ğŸŒŸ **Use Case**

Imagine sitting down after a long day, and your device starts playing music that understands your mood. Whether you're happy, sad, or excitedâ€”this app recommends songs that vibe with how you feel, just from your expressions.

---

## ğŸ“Š **Folder Structure**
```
Emotion-Based-Music-Player/
â”œâ”€â”€ data_collection.py
â”œâ”€â”€ data_training.py
â”œâ”€â”€ inference.py
â”œâ”€â”€ music.py
â”œâ”€â”€ model1.h5
â”œâ”€â”€ labels.npy
â”œâ”€â”€ emotion.npy
â”œâ”€â”€ *.npy (emotion datasets)
```

---
